STATS 101C Final Report: Predicting the Winner of a Basketball Game
Team: JJ ~ Josh Park, JungHwan Park, Mahmood Baggia

Data:
As we started to clean the data, we focused on eliminating duplicate variables and formatting everything to make our results more precise. We deleted the “VT-named” (Visiting Team) variables because those have no correlation to HT- wins. We also dropped “ID” and “Game ID” variables which were not needed.  Since we noticed a significant increase of the prediction level when considering the players abilities, we normalized all those values and calculated the sum of all their z-scores. We also changed all of the categorical variables by using one-hot-encoding where the integer encoded variable is removed and a new binary variable is added for each unique integer value. We thought the “date” variable which was originally just consecutive numeric values of year, month, and date combined was wrongly affecting the response variable so we changed those to days (Monday, Tuesday, …, Sunday) to get the right variable. 

Model:
We used several methods to determine the highest accuracy possible. We started off using the Decision Tree method, knowing that it generally shows a lower level of accuracy compared to other methods. Using the “party” and “partykit” packages, we observed the expected low accuracy results. We then used the more common method, “Random Forest”, that also has a reputation for having a high accuracy. We received the highest accuracy result using this method as expected with 66% accuracy. Following these two methods, we also attempted to use “Ensemble” and “Support Vector Machine (SVM)” which both gave us a lower level of  accuracy at around 64%. Random forest reduces the variance part of error instead of the bias part, so when given a training data set the decision tree method might be more accurate than a random forest. However, on an unexpected validation data set, we observe that Random forest always wins in terms of accuracy.

Overview of Results:
The highest score we submitted was 0.66747. Although we tried other approaches to ensure the highest accuracy after this submission, we were unable to attain a higher score. The random forest method worked well for us because it helps us identify the most important variables from the training dataset and it also helps avoid the problem of overfitting. 
